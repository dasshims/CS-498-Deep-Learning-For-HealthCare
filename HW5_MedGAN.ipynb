{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069b5887",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "069b5887",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a75a98671f121a252e9074ae86f876e5",
     "grade": false,
     "grade_id": "cell-e744ed1b5809a6fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# HW Medical Generative Adversarial Nets (MedGAN)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework, you will get introduced to [MedGAN (medical generative adversarial networks)](http://proceedings.mlr.press/v68/choi17a/choi17a.pdf). MedGAN was proposed to learn from electronic healthcare records (EHRs) and then generate synthetic EHRs. The main reason to do so is to circumvent private issues when sharing sensitive medical records to the public. Take our course for example, we synthesize MIMIC-III data for supporting our course coding assignments. The original version of MedGAN only does *unconditional* generation thus is unable to generate patient records with the specific desired property. In this homework, we will make a slight adaption of MedGAN to form a *conditional* MedGAN that is able to generate patient EHRs who are probably diagnosed with heart failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257e3d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2257e3d5",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14f613d1aec75d3a025c8b5ad6e5d9a4",
     "grade": false,
     "grade_id": "cell-c4713a38b23a9a65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## About Raw Data\n",
    "\n",
    "We will use a dataset synthesized from [MIMIC-III](https://mimic.physionet.org/gettingstarted/access/).\n",
    "\n",
    "The data has been preprocessed for you. Let us load them and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ed22fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "39ed22fb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "787ef941c7a0aeb12e4e01694dd345b8",
     "grade": false,
     "grade_id": "cell-8a60df0209e82e03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set random seed for reproducibility\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "os.environ['PYTHONASHSEED'] = str(seed)\n",
    "\n",
    "# define data set path\n",
    "DATA_PATH = '../HW5_MedGAN-lib/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "886d4b9c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "886d4b9c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab7e0a5351f10adcbfc6899165cf22e7",
     "grade": false,
     "grade_id": "cell-561a0cb8fbdb04b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'train/pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'train/vids.pkl'), 'rb'))\n",
    "hfs = pickle.load(open(os.path.join(DATA_PATH,'train/hfs.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'train/seqs.pkl'), 'rb'))\n",
    "types = pickle.load(open(os.path.join(DATA_PATH,'train/types.pkl'), 'rb'))\n",
    "rtypes = pickle.load(open(os.path.join(DATA_PATH,'train/rtypes.pkl'), 'rb'))\n",
    "\n",
    "assert len(pids) == len(vids) == len(hfs) == len(seqs) == 1000\n",
    "assert len(types) == 619"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a7ea9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ec2a7ea9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4efa4cc3f0a8fb8d4fea8f83fe9f89ca",
     "grade": false,
     "grade_id": "cell-1a40a6dc1471c660",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "where\n",
    "\n",
    "- `pids`: contains the patient ids\n",
    "- `vids`: contains a list of visit ids for each patient\n",
    "- `hfs`: contains the heart failure label (0: normal, 1: heart failure) for each patient\n",
    "- `seqs`: contains a list of visit (in ICD9 codes) for each patient\n",
    "- `types`: contains the map from ICD9 codes to ICD-9 labels\n",
    "- `rtypes`: contains the map from ICD9 labels to ICD9 codes\n",
    "\n",
    "Let us take a patient as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5577251",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b5577251",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b0375cc53e5306aecdb1804f6387a73",
     "grade": false,
     "grade_id": "cell-3e9c3e89c1e210e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "fdfbdb55-8115-4b7b-9a01-cbf01437fd50"
   },
   "outputs": [],
   "source": [
    "# take the 3rd patient as an example\n",
    "print(\"Patient ID:\", pids[3])\n",
    "print(\"Heart Failure:\", hfs[3])\n",
    "print(\"# of visits:\", len(vids[3]))\n",
    "for visit in range(len(vids[3])):\n",
    "    print(f\"\\t{visit}-th visit id:\", vids[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis labels:\", seqs[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", [rtypes[label] for label in seqs[3][visit]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac57b936",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ac57b936",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af69731be7885c4ed976910a743e54c2",
     "grade": false,
     "grade_id": "cell-f3bf22ee226282de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c6ab826d-47e2-4799-e618-971b858ff143"
   },
   "outputs": [],
   "source": [
    "print(\"number of heart failure patients:\", sum(hfs))\n",
    "print(\"ratio of heart failure patients: %.2f\" % (sum(hfs) / len(hfs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace72679",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ace72679",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f988f259a45551a299afe790b2f6b712",
     "grade": false,
     "grade_id": "cell-6040b080dbe251f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1 Build Dataloader\n",
    "\n",
    "### 1.1 CustomDataset\n",
    "\n",
    "First of all, let's implement a custom dataset using PyTorch class `Dataset`, which will characterize the key features of the dataset we want to generate.\n",
    "\n",
    "We will use the sequences of diagnosis codes `seqs` as input and heart failure `hfs` as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7fb9129",
   "metadata": {
    "deletable": false,
    "id": "a7fb9129"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, seqs, hfs):\n",
    "        '''\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "\n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        '''\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        self.x = seqs\n",
    "        self.y = hfs\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        '''\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        '''\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "\n",
    "dataset = CustomDataset(seqs, hfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "371697a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "371697a2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57beb0999b8eca0b4ad7bf01637a2e04",
     "grade": true,
     "grade_id": "cell-511f821093226a7f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "dataset = CustomDataset(seqs, hfs)\n",
    "assert len(dataset) == 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b549ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "47b549ec",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c5ce67b8f3ecc659633ff06f955199b",
     "grade": false,
     "grade_id": "cell-01c84d91d41635bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Data Collator\n",
    "\n",
    "After building the dataset, we shall build a data collator.\n",
    "\n",
    "This collate function `Collator()` will be called by `DataLoader` after fetching a list of samples using the indices from `CustomDataset` to collate the list of samples into batches.\n",
    "\n",
    "For example, when the `DataLoader` gets a list of two samples.\n",
    "\n",
    "```\n",
    "[ [ [0, 1, 2], [3, 0] ], \n",
    "  [ [1, 3, 6, 3], [2], [3, 1] ] ]\n",
    "```\n",
    "\n",
    "where the first sample has two visits `[0, 1, 2]` and `[3, 0]` and the second sample has three visits `[1, 3, 6, 3]`, `[2]`, and `[3, 1]`.\n",
    "\n",
    "The collate function `Collator()` is supposed to concatenate all visits of one patient together to form the inputs for MedGAN, as\n",
    "\n",
    "```\n",
    "[[0, 1, 2, 3 ,0],\n",
    " [1, 3, 6, 3, 2, 3 ,1]]\n",
    "\n",
    "```\n",
    "\n",
    "Further, we transform this to a multi-hot vector representing the appearances of events. Suppose we the number of all possible events is 6, the yielded outputs should be\n",
    "\n",
    "```\n",
    "[[0, 1, 1, 1, 0, 0],\n",
    " [0, 1, 1, 0, 0, 1]]\n",
    "```\n",
    "which will be the final inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e688ba65",
   "metadata": {
    "deletable": false,
    "id": "e688ba65"
   },
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, total_number_of_codes):\n",
    "        self.max_num_codes = total_number_of_codes\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        '''TODO: flatten the input sequence samples into a multi-hot diagnosis codes,\n",
    "        e.g., a multi-hot codes [1, 0, 1] indicates the appearance of [code1, code3] in this patient's all visits.\n",
    "        Arguments:\n",
    "            data: a list of samples fetched from 'CustomDataset'\n",
    "\n",
    "        Outputs:\n",
    "            x: a tensor of shape (# patients, max # diagnosis codes) with torch.float\n",
    "            y: a tensor of shape (# patients, ) with type torch.float\n",
    "        '''\n",
    "        sequences, labels = zip(*data)\n",
    "        num_patients = len(sequences)\n",
    "        max_num_codes =  self.max_num_codes\n",
    "        y = torch.tensor(labels, dtype=torch.float)\n",
    "        x = torch.zeros((num_patients, max_num_codes), dtype=torch.float)\n",
    "        \n",
    "        for i_patient, patient in enumerate(sequences):\n",
    "            '''TODO: Update `x` by looping over each patient.\n",
    "            '''\n",
    "            # your code here\n",
    "            #raise NotImplementedError\n",
    "            flat_codes = set([code for visit in patient for code in visit])\n",
    "            x[i_patient, list(flat_codes)] = 1.0\n",
    "\n",
    "        return x, y\n",
    "\n",
    "collate_fn = Collator(len(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b39f3ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4b39f3ba",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9d71865e60886636174071371bd5e43",
     "grade": true,
     "grade_id": "cell-46a90e9b50777555",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = CustomDataset(seqs, hfs)\n",
    "collate_fn = Collator(len(types))\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn, shuffle=False)\n",
    "loader_iter = iter(loader)\n",
    "x, y = next(loader_iter)\n",
    "\n",
    "assert x.dtype == torch.float\n",
    "assert y.dtype == torch.float\n",
    "assert x.shape == (10, 619)\n",
    "assert y.shape == (10,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0442ba2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c0442ba2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "864052025af1cc47c62e867c64286f00",
     "grade": false,
     "grade_id": "cell-ff7d63b0fbe55ce0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2 Naive AutoEncoder\n",
    "\n",
    "Let's implement a naive AutoEncoder as done in original MedGAN. The first stage is to learn an AutoEncoder by taking reconstruction of the input $x$ by predicting $\\hat{x}$.\n",
    "\n",
    "<img src=\"img/medgan.png\" width=\"400\" />\n",
    "\n",
    "As done in `Dataset` and `DataLoader`, the input $x$ is a binary vector with size (batch_size, # diag codes). \n",
    "\n",
    "We can make a simple encoder with one hidden linear layer `nn.Linear` by transforming $x$ to representations $h$ with size (batch size, hidden dimension). Then, we make a decoder also with one hidden linear layer `nn.Linear` by transforming $h$ to $\\hat{x}$ with the same size as $x$. \n",
    "\n",
    "We will take `nn.Sigmoid` as the prediction activation to map logits $\\hat{x}$ to $[0,1]$.\n",
    "\n",
    "The detailed model architecture for you to follow is shown in the table below.\n",
    "\n",
    "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
    "--- | --- | --- | ---\n",
    "fully connected | input size **input_dim**, output size **hidden_dim** | Tanh | (batch_size, hidden_dim)\n",
    "fully connected | input size **hidden_dim**, output size **input_dim** | Sigmoid | (batch_size, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4c43a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "08e4c43a",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67c5e49a0a56395bc8f0727b43918681",
     "grade": false,
     "grade_id": "cell-1e79a2a9fc351d91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Build the AutoEncoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34ad6e08",
   "metadata": {
    "deletable": false,
    "id": "34ad6e08"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''TODO:\n",
    "        initialize an auto-encoder\n",
    "        self.encoder: linear - tanh activation\n",
    "        self.decoder: linear - sigmoid activation\n",
    "        \n",
    "        Note: try to use nn.Sequential to stack layers and assign the block to self.encoder and self.decoder.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        \n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh()  # Tanh activation\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        '''TODO:\n",
    "        take the input patient records, encode them into hidden representations\n",
    "        using the encoder.\n",
    "        Arguments:\n",
    "            x: the patient records with shape (batch_size, max # diagnosis codes)\n",
    "        Outputs:\n",
    "            h: the encoded representations with shape (batch_size, hidden_dim)\n",
    "        '''\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        h = self.encoder(x)\n",
    "        return h\n",
    "    \n",
    "    def decode(self, h):\n",
    "        '''TODO:\n",
    "        take the input hidden representations, output the reconstructed patient records\n",
    "        using the decoder.\n",
    "        Arguments:\n",
    "            h: the encoded representations with shape (batch_size, hidden_dim)\n",
    "        Outputs:\n",
    "            x: the patient records with shape (batch_size, max # diagnosis codes)\n",
    "        '''\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        x = self.decoder(h)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''TODO:\n",
    "        call the self.encode and self.decode and finally output the reconstructed input x.\n",
    "        Arguments:\n",
    "            x: the patient records with shape (batch_size, max # diagnosis codes)\n",
    "        Outputs:\n",
    "            x: the reconstructed patient records with shape (batch_size, max # diagnosis codes)\n",
    "        '''\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        h = self.encode(x)\n",
    "        x_reconstructed = self.decode(h)\n",
    "        return x_reconstructed\n",
    "\n",
    "model = AutoEncoder(1000, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e04f34f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e04f34f5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f746b861450b5027b61fd3ef2c231118",
     "grade": true,
     "grade_id": "cell-83c16dcf42a35b42",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "x = torch.randint(0, 2, (32, 1000)).float()\n",
    "h = model.encode(x)\n",
    "x_hat = model.decode(h)\n",
    "assert isinstance(model.encoder, nn.Sequential), 'You should implement your encoder using nn.Sequential, found {}.'.format(type(model.encoder))\n",
    "assert isinstance(model.decoder, nn.Sequential), 'You should implement your decoder using nn.Sequential, found {}.'.format(type(model.decoder))\n",
    "assert h.shape == torch.Size([32, 256]), 'The encoder output shape is wrong, expect [32, 256], got {}'.format(h.shape)\n",
    "assert x_hat.shape == torch.Size([32, 1000]), 'The decoder output shape is wrong, expect [32, {}], got {}'.format(1000,x_hat.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3741a00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d3741a00",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3002e0cf155587fff491f0a1012ecaa",
     "grade": false,
     "grade_id": "cell-b1d786283dde095a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Train the AutoEncoder model\n",
    "\n",
    "With the built AE model at hand, it is easy to follow the common practice to train AE using reconstruction loss.\n",
    "\n",
    "Let's make use of the completed `CustomDataset`, `Collator`, and `AutoEncoder` to achieve this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "673dbe2f",
   "metadata": {
    "deletable": false,
    "id": "673dbe2f",
    "outputId": "4f90fb1a-8b11-40d0-c8f3-03f6723beaaa"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Define the optimizer (Adam) with learning rate 1e-3.\n",
    "Define the loss_fn (loss function, nn.BCELoss).\n",
    "Do the training in each iteration by\n",
    "- forward ae model to get x_hat\n",
    "- compute reconstruction loss\n",
    "- call loss.backward\n",
    "- update parameters using optimizer.step\n",
    "'''\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = CustomDataset(seqs, hfs)\n",
    "collate_fn = Collator(len(types))\n",
    "dataloader = DataLoader(dataset, batch_size=128, collate_fn=collate_fn)\n",
    "ae = AutoEncoder(input_dim=len(types), hidden_dim=256)\n",
    "\n",
    "optimizer, loss_fn = None, None\n",
    "# your code here\n",
    "#raise NotImplementedError\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "loss_list = []\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for (x,y) in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        x_hat = ae(x)\n",
    "        loss = loss_fn(x_hat, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    loss_list.append(epoch_loss)\n",
    "    print(f'epoch {epoch} training autoencoer loss {epoch_loss}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list, label='autoencoder loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('AE loss during training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e409944c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e409944c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60fa272b29e6897b5e3f2805cb89fb48",
     "grade": true,
     "grade_id": "cell-3adce8fbfe82cd16",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19550e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9f19550e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "152659c96c159f7afe01287b9f638755",
     "grade": false,
     "grade_id": "cell-51b06809badd8dba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3 MedGAN [100 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ba920",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ea3ba920",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ebc581349b3038feb96002c55879349",
     "grade": false,
     "grade_id": "cell-4aa1db90b8f5a0bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.1 Build the Generator and Discrminator [30 points]\n",
    "\n",
    "Next, we will construct the generator and discriminator of the MedGAN model.\n",
    "\n",
    "Note that we take a conditional GAN here, the input of generator should be the concatenation of $z$ vector and the condition $y$. Meanwhile, the input of discriminator is the concatenation of $x$ vector and the condition $y$.\n",
    "\n",
    "<img src=\"img/conditional-gan.png\" width=\"400\"/>\n",
    "\n",
    "This figure is drawn from the paper [Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1411.1784.pdf). In our case, the condition $y$ is a value in $\\{0,1\\}$ indicating whether the patient has heart failure or not.\n",
    "\n",
    "\n",
    "The architecture details are as follows.\n",
    "\n",
    "**Generator**\n",
    "\n",
    "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
    "--- | --- | --- | ---\n",
    "batchnorm1d | - | - | (batch_size, input_dim)\n",
    "ReLU | - | - |  (batch_size, input_dim)\n",
    "fully connected | input size **input_dim**, output size **hidden_dim** | - | (batch_size, hidden_dim)\n",
    "batchnorm1d | - | - | (batch_size, hidden_dim)\n",
    "Tanh | - | - |  (batch_size, hidden_dim)\n",
    "fully connected | input size **hidden_dim**, output size **hidden_dim** | - | (batch_size, hidden_dim)\n",
    "\n",
    "\n",
    "**Discrminator**\n",
    "\n",
    "Layers | Configuration | Activation Function | Output Dimension (batch, feature)\n",
    "--- | --- | --- | ---\n",
    "fully connected | input size **input_dim**, output size **hidden_dim** | - | (batch_size, hidden_dim)\n",
    "ReLU | - | - |  (batch_size, input_dim)\n",
    "fully connected | input size **hidden_dim**, output size **1** | - | (batch_size, 1)\n",
    "Sigmoid | - | - |  (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4343c4e",
   "metadata": {
    "deletable": false,
    "id": "e4343c4e"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''input records and labels for conditional generation\n",
    "        \n",
    "        TODO: define the layer components as shown in the above table.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.linear1 = None\n",
    "        self.bn1 = None\n",
    "        self.act1 = None\n",
    "        self.linear2 = None\n",
    "        self.bn2 = None\n",
    "        self.act2 = None\n",
    "        \n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(input_dim)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.act2 = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, z, y):\n",
    "        '''\n",
    "        Arguments:\n",
    "            z: input random noise with shape (n, hidden_dim)\n",
    "            y: input conditional y with shape (n,)\n",
    "        \n",
    "        Outputs:\n",
    "            h: the generated representation h with shape (n, hidden_dim)\n",
    "        \n",
    "        TODO: take the defined components to do forward inference.\n",
    "        \n",
    "        Note: do not forget to take the *residual connection* for each layer as described in MedGAN paper,\n",
    "        i.e., (z,y) -> tmp -> layer1(bn1+act1+linear1) -> h -> h = h + z -> bn2 -> z -> layer2(act2+linear2) -> h -> h = h+z\n",
    "        \n",
    "        '''\n",
    "        # concatenate the input z and condition y\n",
    "        tmp = torch.cat([z, y[:,None]], 1)\n",
    "        \n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        \n",
    "        h = self.linear1(self.act1(self.bn1(tmp)) + tmp)\n",
    "        \n",
    "        tmp = h\n",
    "        \n",
    "        h = self.linear2(self.act2(self.bn2(h)) + tmp)\n",
    "\n",
    "        return h\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''input records and labels for conditional discrimination\n",
    "        \n",
    "        TODO: define the layer components as shown in the above table.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.linear1 = None\n",
    "        self.act1 = None\n",
    "        self.linear2 = None\n",
    "        self.act2 = None\n",
    "        \n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, 1) \n",
    "        self.act2 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        Arguments:\n",
    "            x: input records with shape (n, input_dim)\n",
    "            y: input conditional labels with shape (n,)\n",
    "        Outputs:\n",
    "            out: the predicted probability if input x is real or fake samples in shape (n,)\n",
    "        \n",
    "        Note: unlike in Generator, we DO NOT take residual connection here.\n",
    "        '''\n",
    "        \n",
    "        # concatenate the input x and condition y\n",
    "        x = torch.cat([x, y[:,None]], axis=1)\n",
    "        out = None\n",
    "        \n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        \n",
    "        h = self.act1(self.linear1(x))\n",
    "        out = self.act2(self.linear2(h))\n",
    "        \n",
    "        return out.squeeze(1)\n",
    "\n",
    "generator = Generator(100, 99)\n",
    "discriminator = Discriminator(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a0d70e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7a0d70e9",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45b8f0410ba7bff9e47e8288b53c99fc",
     "grade": true,
     "grade_id": "cell-ab81243260183827",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "assert generator.linear1.in_features==100 and generator.linear1.out_features==99\n",
    "assert generator.bn1.num_features==100\n",
    "assert isinstance(generator.act1, nn.ReLU)\n",
    "assert generator.linear2.in_features==99 and generator.linear1.out_features==99\n",
    "assert generator.bn2.num_features==99\n",
    "assert isinstance(generator.act2, nn.Tanh)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ebe05",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "730ebe05",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b67dea885373f4f525317211a9bbe863",
     "grade": false,
     "grade_id": "cell-15f5918ff6d92342",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.2 Build the MedGAN model [30 points]\n",
    "\n",
    "\n",
    "Now, finally we come to build our conditional MedGAN with all completed components: AutoEncoder, Generator, Discriminator.\n",
    "\n",
    "Recall the MedGAN architecture as\n",
    "\n",
    "<img src=\"img/medgan.png\" width=\"400\" />\n",
    "\n",
    "In the MedGAN training phase, only the `decoder` of the AutoEncoder model will be used to decode the outputs of the `generator`.\n",
    "\n",
    "Also, we need a `generate` function for `MedGAN` such that it is able to generate fake samples for `discriminator` to classify.\n",
    "\n",
    "In the `forward` function, we need to implement the compution for discriminator and generator loss.\n",
    "\n",
    "$\\ell_{d}= - \\frac1m \\sum_{i=1}^m [log (D(x_i)+\\epsilon) + \\log (1-D(\\hat{x}_i)+\\epsilon)]$\n",
    "\n",
    "$\\ell_{g}= - \\frac1m \\sum_{i=1}^m log (D(\\hat{x}_i)+\\epsilon)$\n",
    "\n",
    "where $x_i$ is the real record and $\\hat{x}_i$ is the fake record generated by generator;\n",
    "\n",
    "$\\epsilon$ is added to avoid numerical issue in the $log$ function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bec658d5",
   "metadata": {
    "deletable": false,
    "id": "bec658d5"
   },
   "outputs": [],
   "source": [
    "class MedGAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        '''The main class for MedGAN model. It consists of three parts:\n",
    "        AutoEncoder\n",
    "        Generator\n",
    "        Discriminator\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        ### DO NOT change the names\n",
    "        self.ae = AutoEncoder(input_dim, hidden_dim)\n",
    "        self.generator = Generator(hidden_dim+1, hidden_dim) # input random noise/representation + label\n",
    "        self.discriminator = Discriminator(input_dim+1, hidden_dim) # input records + label\n",
    "            \n",
    "    def generate(self, n, y):\n",
    "        '''\n",
    "        Arguments:\n",
    "            n: number of fake samples to be generated\n",
    "            y: the condition label used to make conditional generation\n",
    "        Outputs:\n",
    "            x: the generated fake samples with shape (n, self.input_dim)\n",
    "        \n",
    "        TODO: Generate n fake samples using the generator.\n",
    "        \n",
    "        First, sample a random vector z using torch.randn with size (n, self.hidden_dim).\n",
    "        Then, generate the fake encoded representations h using z and y as inputs for self.generator .\n",
    "        Last, generate the fake example x using self.ae.decode function.\n",
    "        '''\n",
    "        \n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        z = torch.randn(n, self.hidden_dim)\n",
    "        h_fake = self.generator(z, y)\n",
    "        x_fake = self.ae.decode(h_fake)\n",
    "        return x_fake\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        '''Take the input x and conditional y, compute the discriminator loss and generator loss\n",
    "        Arguments:\n",
    "            x: input records with shape (n, self.input_dim)\n",
    "            y: input labels with shape (n,)\n",
    "        Outputs:\n",
    "            d_loss: discriminator loss values\n",
    "            g_loss: generator loss values\n",
    "            \n",
    "        TODO: Implement the prediction of fake or real examples using self.discriminator.\n",
    "        Then, compute the discriminator loss and generator loss.\n",
    "        '''\n",
    "        # generate fake samples by putting random noise z into the generator\n",
    "        x_fake = self.generate(len(x), y)\n",
    "        \n",
    "        fake_score = None\n",
    "        real_score = None\n",
    "        \n",
    "        # discriminate x and x_fake using discriminator\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        fake_score = self.discriminator(x_fake, y)\n",
    "        real_score = self.discriminator(x, y)\n",
    "        \n",
    "        # compute generator loss and discriminator loss, take epsilon for numerical stability\n",
    "        d_loss = None\n",
    "        g_loss = None\n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        d_loss = -(torch.log(real_score + epsilon) + torch.log(1 - fake_score + epsilon)).mean()\n",
    "        g_loss = -(torch.log(fake_score + epsilon)).mean()\n",
    "        \n",
    "        return g_loss, d_loss\n",
    "\n",
    "medgan = MedGAN(100, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0baa58c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d0baa58c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9dbebb90b38376c894874993c19ea96",
     "grade": true,
     "grade_id": "cell-3aad9fcdf31045af",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "x = torch.ones((2,100))\n",
    "y = torch.ones((2))\n",
    "out = medgan(x,y)\n",
    "assert isinstance(medgan.ae, AutoEncoder), 'medgan.ae should be AutoEncoder!'\n",
    "assert medgan.generator.linear1.in_features==129\n",
    "assert medgan.generator.linear1.out_features==128\n",
    "assert medgan.discriminator.linear1.in_features==101\n",
    "assert medgan.discriminator.linear1.out_features==128\n",
    "assert isinstance(medgan.generator, Generator), 'medgan.generator should be Generator!'\n",
    "assert isinstance(medgan.discriminator, Discriminator), 'medgan.discriminator should be Discriminator!'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c17309",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "f8c17309",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8af56ea9d12034d1626c5b6510006ae",
     "grade": false,
     "grade_id": "cell-482da0fbeffcad29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.3 Build optimizers for MedGAN [20 points]\n",
    "\n",
    "Now we turn to build optimizers for MedGAN. Note that GAN model trains generator and discriminator in an adversarial paradigm, we need to split their parameters when designing optimizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd5f780b",
   "metadata": {
    "deletable": false,
    "id": "dd5f780b"
   },
   "outputs": [],
   "source": [
    "def build_optimizer(medgan):\n",
    "    '''build two separate optimizers for the generator and discriminator, respectively.\n",
    "    \n",
    "    TODO: add params which belong to AutoEncoder and Generator to g_param_list;\n",
    "    add params which belong to discriminator to d_param_list.\n",
    "    '''\n",
    "    g_param_list, d_param_list = [], []\n",
    "    for name, param in medgan.named_parameters():\n",
    "        # your code here\n",
    "        #raise NotImplementedError\n",
    "        if 'generator' in name or 'ae' in name:  # Adjust based on module names\n",
    "            g_param_list.append(param)\n",
    "        else:\n",
    "            d_param_list.append(param)\n",
    "            \n",
    "    g_optimizer = torch.optim.Adam(g_param_list, lr=1e-4)\n",
    "    d_optimizer = torch.optim.Adam(d_param_list, lr=1e-4)\n",
    "    return g_optimizer, d_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9483c84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d9483c84",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99d4cb3108522198053da25875457e11",
     "grade": true,
     "grade_id": "cell-621a8feff2fbf513",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22cd36e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b22cd36e",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ef25a70e0a1d69207af6f8dca690378",
     "grade": false,
     "grade_id": "cell-2cc9dd019a216589",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.4 Train MedGAN and generate synthetic records [20 points]\n",
    "\n",
    "Finally, we come to train the MedGAN we implement and use it to generate synthetic records we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef4b46b5",
   "metadata": {
    "deletable": false,
    "id": "ef4b46b5",
    "outputId": "37f68760-4958-43c6-e6d5-43b5d1f7ee39"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: complete the part where we use to update parameters of generator by g_loss and parameters of discriminator by d_loss.\n",
    "'''\n",
    "\n",
    "# build dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = CustomDataset(seqs, hfs)\n",
    "collate_fn = Collator(len(types))\n",
    "dataloader = DataLoader(dataset, batch_size=128, collate_fn=collate_fn)\n",
    "\n",
    "# build model\n",
    "medgan = MedGAN(input_dim=len(types), hidden_dim=256)\n",
    "\n",
    "# apply the pretrained ae model to medgan autoencoder model\n",
    "medgan.ae.load_state_dict(ae.state_dict())\n",
    "\n",
    "# build optimizer for generator and discriminator, respectively\n",
    "g_opt, d_opt = build_optimizer(medgan)\n",
    "\n",
    "n_epochs = 40\n",
    "d_train_period = 1\n",
    "g_train_period = 2\n",
    "\n",
    "g_loss_list, d_loss_list = [], []\n",
    "for epoch in range(n_epochs):\n",
    "    medgan.train()\n",
    "    g_loss_all, d_loss_all = 0, 0\n",
    "    for (x,y) in dataloader:\n",
    "        for _ in range(d_train_period):\n",
    "            \n",
    "            # your code here\n",
    "            #raise NotImplementedError\n",
    "\n",
    "            d_opt.zero_grad() \n",
    "            g_loss, d_loss = medgan(x, y)\n",
    "     \n",
    "            d_loss.backward()\n",
    "            d_opt.step()\n",
    "            \n",
    "            d_loss_all += d_loss.item()\n",
    "\n",
    "        for _ in range(g_train_period):\n",
    "            # your code here\n",
    "            #raise NotImplementedError\n",
    "\n",
    "            g_opt.zero_grad() \n",
    "            g_loss, d_loss = medgan(x, y)\n",
    "     \n",
    "            g_loss.backward()\n",
    "            g_opt.step()\n",
    "            \n",
    "            g_loss_all += g_loss.item()\n",
    "    \n",
    "    g_loss_list.append(g_loss_all)\n",
    "    d_loss_list.append(d_loss_all)\n",
    "    print(f'Epoch {epoch} Generator loss {g_loss_all} Discriminator loss {d_loss_all}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e109379a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e109379a",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a922433d80b80d29915d9447d718a2b",
     "grade": false,
     "grade_id": "cell-3363811214e80e72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "3d021d9b-0fa6-4018-8a74-a3f59f58cdeb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(g_loss_list, label='generator loss')\n",
    "plt.plot(d_loss_list, label='discriminator loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('GAN loss during training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fdcb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "969fdcb7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe7a45ed46f06603c6629ab4f1595fac",
     "grade": false,
     "grade_id": "cell-c527e3cd9e753b7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To verify if our MedGAN is able to do conditional generation, we design an experiment like:\n",
    "\n",
    "- Train a heart failure classifier on the real EHRs record from which MedGAN learns.\n",
    "- Use the trained MedGAN to generate synthetic records with given condition in $\\{0,1\\}$\n",
    "- Use the trained classifier to make predictions on synthetic records to see if the predicted outcomes match the given condition.\n",
    "\n",
    "If the predicted outcome matches the given condition well, our MedGAN does a great job because it succeeds to produce records which are fit to the given condition. Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df432b66",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "df432b66",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cce383d077b60abe4efb1d992be294c",
     "grade": false,
     "grade_id": "cell-7f578e67ed15392f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "52b4d636-da9f-4e19-f21f-f495bd5f05c2"
   },
   "outputs": [],
   "source": [
    "# train a classifier on the real records so we can classify if one patient record belongs to heart failure case\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        pred = self.clf(x).squeeze(1)\n",
    "        return pred\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = CustomDataset(seqs, hfs)\n",
    "collate_fn = Collator(len(types))\n",
    "dataloader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn)\n",
    "clf = Classifier(len(types), 256)\n",
    "optimizer = torch.optim.Adam(clf.parameters(), 1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    for (x,y) in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = clf(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'epoch {epoch} training classifier loss {epoch_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79522be5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "79522be5",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dddb1c43b876011df4fa8e95ea3cd540",
     "grade": false,
     "grade_id": "cell-9d81a6a977e2c7f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's generate synthetic records and test! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21518107",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "21518107",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fad48eb3fe6af867e2610a4f2ec72f00",
     "grade": false,
     "grade_id": "cell-6b8a5ab56c24bcf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "9e7b0a60-e921-4e60-eb04-8754fe1dd27d"
   },
   "outputs": [],
   "source": [
    "print('generate 100 synthetic records with/without heart failure')\n",
    "y = torch.cat([torch.ones(50), torch.zeros(50)], 0)\n",
    "medgan.eval()\n",
    "with torch.no_grad():\n",
    "    x = medgan.generate(100, y)\n",
    "    pred = clf(x)\n",
    "\n",
    "\n",
    "print('evaluate how much the generated synthetic records match the given condition:')    \n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y.long().numpy(), pred.numpy())\n",
    "print('auc:', auc)\n",
    "    \n",
    "pred[pred>0.5]=1\n",
    "pred[pred<=0.5]=0\n",
    "acc = (pred == y).float().mean().item()\n",
    "print('accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd1cd4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "17cd1cd4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2062fa31b0aa1a5a81aafc314994da7b",
     "grade": false,
     "grade_id": "cell-9668285bbf290d02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Since training a GAN model has long been \"random\", it is possible that your model doesn't perform so well.\n",
    "The reason might be \n",
    "\n",
    "(1) the training records are so small. Here we only have 1000 records.\n",
    "\n",
    "(2) the conditional GAN does not capture the given condition well.\n",
    "\n",
    "You can look into the references regarding conditional GAN later to think about how to improve conditional GAN for syntheti records generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd00399c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bd00399c",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc40e98dd6a76956a42aa698b7275c64",
     "grade": true,
     "grade_id": "cell-0faac9f30a3ecb71",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a7059697-a415-48f1-9050-a30f1c4bb55d"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b9272",
   "metadata": {
    "id": "9f7b9272"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee96c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b098b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW_MedGAN_pre.ipynb",
   "provenance": []
  },
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/HW5_MedGAN/HW5_MedGAN.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (Threads: 2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
