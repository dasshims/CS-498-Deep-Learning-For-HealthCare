{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "169ef2aa5c802f7e7624d33b906d094c",
     "grade": false,
     "grade_id": "cell-079a6526398a85d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# HW3 Seq2Seq\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework, you will use the sequence-to-sequence (Seq2seq) autoencoder model to generate patient EHR embedding and use these embeddings to conduct unsupervised patient clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba17b08f650c9433337e15531a0ee014",
     "grade": false,
     "grade_id": "cell-e559bcf016c4584e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.782385Z",
     "start_time": "2022-02-21T03:00:13.009701Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79eeb7a0899f28f0059173dffbd10c43",
     "grade": false,
     "grade_id": "cell-662d67c8f89d61b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"../HW3_Seq2Seq-lib/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3c6c5e5e3e2f994e259a64f245cb35c",
     "grade": false,
     "grade_id": "cell-21b2fb2dbf1ff2e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82c9e6d237196ef284db7fc15d363ffc",
     "grade": false,
     "grade_id": "cell-572524c6438cfdcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## About Raw Data\n",
    "\n",
    "We will use the same dataset synthesized from [MIMIC-III](https://mimic.physionet.org/gettingstarted/access/), but with different input formats.\n",
    "\n",
    "The data has been preprocessed for you. Let us load them and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.796607Z",
     "start_time": "2022-02-21T03:00:13.784824Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e525ed6e8d92afb4dfc57c5604348da9",
     "grade": false,
     "grade_id": "cell-dd5e81b9b8c115dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'train/pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'train/vids.pkl'), 'rb'))\n",
    "hfs = pickle.load(open(os.path.join(DATA_PATH,'train/hfs.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'train/seqs.pkl'), 'rb'))\n",
    "types = pickle.load(open(os.path.join(DATA_PATH,'train/types.pkl'), 'rb'))\n",
    "rtypes = pickle.load(open(os.path.join(DATA_PATH,'train/rtypes.pkl'), 'rb'))\n",
    "\n",
    "assert len(pids) == len(vids) == len(hfs) == len(seqs) == 1000\n",
    "assert len(types) == 619"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c812334263be9acfbda6331f22b177de",
     "grade": false,
     "grade_id": "cell-8ea7106e7baf916e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "where\n",
    "\n",
    "- `pids`: contains the patient ids\n",
    "- `vids`: contains a list of visit ids for each patient\n",
    "- `hfs`: contains the heart failure label (0: normal, 1: heart failure) for each patient\n",
    "- `seqs`: contains a list of visit (in ICD9 codes) for each patient\n",
    "- `types`: contains the map from ICD9 codes to ICD-9 labels\n",
    "- `rtypes`: contains the map from ICD9 labels to ICD9 codes\n",
    "\n",
    "Let us take a patient as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.805103Z",
     "start_time": "2022-02-21T03:00:13.798856Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d1e011b0a32250f45ba3d890c1011a1",
     "grade": false,
     "grade_id": "cell-c5e10739d77ed1fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# take the 3rd patient as an example\n",
    "\n",
    "print(\"Patient ID:\", pids[3])\n",
    "print(\"Heart Failure:\", hfs[3])\n",
    "print(\"# of visits:\", len(vids[3]))\n",
    "for visit in range(len(vids[3])):\n",
    "    print(f\"\\t{visit}-th visit id:\", vids[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis labels:\", seqs[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", [rtypes[label] for label in seqs[3][visit]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79b3ccb1db159b6ab715e14051a1f064",
     "grade": false,
     "grade_id": "cell-bca6680fe2226e07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that `seqs` is a list of list of list. That is, `seqs[i][j][k]` gives you the k-th diagnosis codes for the j-th visit for the i-th patient.\n",
    "\n",
    "And you can look up the meaning of the ICD9 code online. For example, `DIAG_276` represetns *disorders of fluid electrolyte and acid-base balance*.\n",
    "\n",
    "Further, let see number of heart failure patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.813558Z",
     "start_time": "2022-02-21T03:00:13.809892Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80b5edd187b2c48d6dd86f2e57480348",
     "grade": false,
     "grade_id": "cell-73c9c7065254f3e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"number of heart failure patients:\", sum(hfs))\n",
    "print(\"ratio of heart failure patients: %.2f\" % (sum(hfs) / len(hfs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad811c829262a858b528170e34ce0b38",
     "grade": false,
     "grade_id": "cell-9797093f8f141582",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the sake of computational efficiency, we will only use the diagnosis that appears more than or equal to 50 times in `seqs`. We need first store these frequent diagnosis labels into list `freq_codes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.828857Z",
     "start_time": "2022-02-21T03:00:13.815625Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7b05e5800d3bb8949ac33b2cbc639bc",
     "grade": false,
     "grade_id": "cell-b18774a2ebec35dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "freq_codes = []\n",
    "\n",
    "'''\n",
    "Append all codes that appears more than 50 times in freq_codes list.\n",
    "'''\n",
    "\n",
    "cnt_dict = {}\n",
    "for i in range(len(seqs)):\n",
    "    for j in range(len(seqs[i])):\n",
    "        for each_code in seqs[i][j]:\n",
    "            if each_code not in cnt_dict:\n",
    "                cnt_dict[each_code] = 1\n",
    "            else:\n",
    "                cnt_dict[each_code] += 1\n",
    "\n",
    "for each_code in cnt_dict:\n",
    "    if cnt_dict[each_code] >= 50:\n",
    "        freq_codes.append(each_code)\n",
    "        \n",
    "assert len(freq_codes) == 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.834263Z",
     "start_time": "2022-02-21T03:00:13.830853Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "759dc445a5705fc80f7b08ca48768dcc",
     "grade": false,
     "grade_id": "cell-4045ed0f1f4b9687",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "code2idx = {code: idx for idx, code in enumerate(freq_codes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4ee665c6bdfc4a40df0d7adc8c7bf06",
     "grade": false,
     "grade_id": "cell-2a4a520635a7d2b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2 Build the dataset [20 points]\n",
    "\n",
    "### 2.1 Custom the dataset  [5 points]\n",
    "\n",
    "First, let us implement a custom dataset using PyTorch class `Dataset`, which will characterize the key features of the dataset we want to generate.\n",
    "\n",
    "We will use the sequences of diagnosis codes `seqs` as input and heart failure `hfs` as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.842268Z",
     "start_time": "2022-02-21T03:00:13.836848Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, hfs):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        self.x = seqs \n",
    "        self.y = hfs\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        input_seq = self.x[index]  # Get input sequence for the selected patient\n",
    "        label = self.y[index]      # Get heart failure label for the selected patient\n",
    "        \n",
    "        return input_seq, label\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(seqs, hfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.855246Z",
     "start_time": "2022-02-21T03:00:13.844860Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97dfe0b6efd5682c49cd7f541b05ba37",
     "grade": true,
     "grade_id": "cell-2cdf5a1e3b59dd72",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "dataset = CustomDataset(seqs, hfs)\n",
    "assert len(dataset) == 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c7377e1f1b8ce10a06da315645316b5",
     "grade": false,
     "grade_id": "cell-c9b3a70f18db1272",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Collate Function [10 points]\n",
    "\n",
    "As you note that, we do not convert the data to tensor in the built `CustomDataset`. Instead, we will do this using a collate function `collate_fn()`. \n",
    "\n",
    "This collate function `collate_fn()` will be called by `DataLoader` after fetching a list of samples using the indices from `CustomDataset` to collate the list of samples into batches.\n",
    "\n",
    "For example, assume the `DataLoader` gets a list of two samples.\n",
    "\n",
    "```\n",
    "[ [ [0, 1, 2], [4, 0] ], \n",
    "  [ [2, 3], [1], [1, 4] ] ]\n",
    "```\n",
    "\n",
    "where the first sample has two visits `[0, 1, 2]` and `[4, 0]` and the second sample has three visits `[2, 3]`, `[1]`, and `[4, 1]`.\n",
    "\n",
    "The collate function `collate_fn()` is supposed to pad them into a multi-hot vector with shape (2, 3, 5), where 3 is the maximum number of visits, and 5 is the number of total diagnosis codes.\n",
    "\n",
    "``` \n",
    "[ [ [1, 1, 1, 0, 0], [1, 0, 0, 0, 1], [0, 0, 0, 0, 0] ],\n",
    "  [ [0, 0, 1, 1, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 1] ] ]\n",
    "```\n",
    "\n",
    "Further, the padding information will be stored in a mask with the shape (2, 3), where 1 indicates that the visit at this position is from the original input, and 0 indicates that the visit at this position is the padded value.\n",
    "\n",
    "```\n",
    "[ [1, 1, 0], \n",
    "  [1, 1, 1] ]\n",
    "```\n",
    "\n",
    "We need to pad the sequences into the same length so that we can do batch training on GPU. And we also need this mask so that when training, we can ignored the padded value as they actually do not contain any information.\n",
    "\n",
    "We only keep the codes that in the `freq_codes` list and drop other codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.865460Z",
     "start_time": "2022-02-21T03:00:13.857942Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, len(freq_codes)). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, len(freq_codes)) of type torch.float\n",
    "        masks: a tensor of shape (# patiens, max # visits) of type torch.bool\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    max_num_visits = max(num_visits)\n",
    "    \n",
    "    masks = torch.zeros((num_patients, max_num_visits), dtype=torch.bool)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, len(freq_codes)), dtype=torch.float)    \n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            for code in visit:\n",
    "                \"\"\"\n",
    "                TODO: 1. check if code is in freq_codes;\n",
    "                      2. obtain the code index using code2idx;\n",
    "                      3. set the correspoindg element in x to 1.\n",
    "                \"\"\"\n",
    "                # your code here\n",
    "                #print(freq_codes)\n",
    "                if code in freq_codes:\n",
    "                    #print(f\"code {code}\")\n",
    "                    code_index = code2idx[code]\n",
    "                    x[i_patient, j_visit, code_index] = 1\n",
    "            masks[i_patient, j_visit] = True\n",
    "    \n",
    "    masks = torch.sum(x, dim=-1) > 0\n",
    "    \n",
    "    return x, masks, y\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.890164Z",
     "start_time": "2022-02-21T03:00:13.874219Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "114e688a481d333d8a9d0164f97c3740",
     "grade": true,
     "grade_id": "cell-50005521abaf98fe",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, y = next(loader_iter)\n",
    "\n",
    "assert x.dtype == torch.float\n",
    "assert y.dtype == torch.float\n",
    "assert masks.dtype == torch.bool\n",
    "\n",
    "assert x.shape == (10, 3, 105)\n",
    "assert y.shape == (10,)\n",
    "assert masks.shape == (10, 3)\n",
    "\n",
    "assert x[0][0].sum() == 9\n",
    "assert masks[0].sum() == 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec4aad114c789bb380ac0fc2617808ce",
     "grade": false,
     "grade_id": "cell-0fb787270b9161d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we have `CustomDataset` and `collate_fn()`. Let us split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.896912Z",
     "start_time": "2022-02-21T03:00:13.891937Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfdd21ad2e22810481570104a76ae7c7",
     "grade": false,
     "grade_id": "cell-4f2e9e518517db0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5c40e75413b983f88fc5b0016b39c5f",
     "grade": false,
     "grade_id": "cell-dbe40dc6b384c8f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 DataLoader [5 points]\n",
    "\n",
    "Now, we can load the dataset into the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.903477Z",
     "start_time": "2022-02-21T03:00:13.899171Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.912143Z",
     "start_time": "2022-02-21T03:00:13.906737Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46bcf6f9683a51a89b6acb0baac4eb5d",
     "grade": true,
     "grade_id": "cell-f9a203ff240d95cf",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)\n",
    "assert len(train_loader) == 25, \"Length of train_loader should be 25, instead we got %d\"%(len(train_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "188eea48a999a0ff69f774e012ade42b",
     "grade": false,
     "grade_id": "cell-d852f17eea849e99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3 Building the Seq2Seq model [50 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55642711105f694be3b52b46e43579e3",
     "grade": false,
     "grade_id": "cell-f206e1c0d9459abb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The most common sequence-to-sequence (Seq2Seq) models are *encoder-decoder* models, which commonly use a *recurrent neural network* (RNN) to *encode* the source (input) sentence into a single vector. In this notebook, we'll refer to this single vector as a *context vector*. Since we have implemented RNNs in the previous homework, we will instead use a CNN as the encoder. We can think of the context vector as being an abstract representation of the entire input sentence. This vector is then *decoded* to output the target by generating it one word at a time.\n",
    "\n",
    "In this question, we will build a naive Seq2Seq autoencoder model which uses a CNN with attention mechanism as encoder and a GRU as decoder. The input to the encoder will be the multi-hot code vector at each timestep. The decoder will try to reconstruct the input at each timestep based on the hidden embedding of the encoder network. In this way, we can store the longitudinal patient EHR data in a vector (i.e., the hidden state of the encoder network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e266ec793f3ff6754af0e53368e3b70e",
     "grade": false,
     "grade_id": "cell-35b60df3b95c0443",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.1 Build the encoder model [15 points]\n",
    "\n",
    "First, we will build the encoder model using ```nn.Conv2d```. We will 1) feed the input into the CNN and calculate a hidden state for each timestep, 2) calculate the attention weights for each timestep, 3) calculate the context vector by aggregating all the hidden states using the attention weights. Importantly, you need to use `masks` to mask out the visits when you calculate the attention weights and aggregate the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.921292Z",
     "start_time": "2022-02-21T03:00:13.914355Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Define the CNN using `nn.Conv2d()`.\n",
    "               The input channel is 1. Set the output channel to 128.\n",
    "               Set kernel size to (3, 105). Set padding to (1,0).\n",
    "            2. Define the attention layer using `nn.Linear`.\n",
    "        \"\"\"\n",
    "#         self.cnn = None\n",
    "#         self.att = None\n",
    "        # your code here\n",
    "        self.cnn = nn.Conv2d(1, 128, kernel_size=(3, 105), padding=(1, 0))\n",
    "        self.att = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Pass the sequence through the CNN layer and get hidden states.\n",
    "               You may need to add an extra dimension at dim=1.\n",
    "            2. Permute and squeeze the hidden states, so that the shape is (batch_size, # visits, 128)\n",
    "            3. Calculate the attention score using `self.att`\n",
    "            4. Mask out the padded visits in the attention score with -1e9.\n",
    "            5. Perform softmax on the attention score to get the attention value.\n",
    "            6. Calculate the context vector using attention value and hidden states.\n",
    "            \n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits)\n",
    "\n",
    "        Outputs:\n",
    "            states: context vectors of shape (batch_size, 128)\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "\n",
    "#         cnn_output = self.cnn(x.unsqueeze(1))  # Add extra dimension at dim=1\n",
    "#         cnn_output = F.relu(cnn_output)\n",
    "#         print(cnn_output[0][0])\n",
    "#         hidden_states = cnn_output.permute(0, 2, 1, 3).squeeze(3)\n",
    "#         print(hidden_states[0][0])\n",
    "#         attn_scores = self.att(hidden_states)\n",
    "#         print(attn_scores)\n",
    "        \n",
    "#         attn_scores_masked = attn_scores[masks.unsqueeze(2)] = -1e9\n",
    "#         print(\"attn_scores_masked\")\n",
    "#         print(attn_scores_masked)\n",
    "#         attention_value = nn.functional.softmax(attn_scores_masked, dim=1)\n",
    "#         print(attn_weights[0][0])\n",
    "#         context_vector = torch.sum(attention_value.unsqueeze(2) * hidden_states, dim=1)\n",
    "        \n",
    "#         print(context_vector)\n",
    "\n",
    "        hidden_states = self.cnn(x.unsqueeze(1))  # Add extra dimension at dim=1\n",
    "        \n",
    "        # Permute and squeeze the hidden states to desired shape\n",
    "        hidden_states = hidden_states.permute(0, 2, 1, 3).squeeze(3)\n",
    "        \n",
    "        # Calculate the attention score using self.att\n",
    "        attention_score = self.att(hidden_states)\n",
    "        \n",
    "        # Mask out the padded visits in the attention score\n",
    "        attention_score[masks.unsqueeze(2)] = -1e9\n",
    "        \n",
    "        # Perform softmax on the attention score to get the attention value\n",
    "        attention_value = nn.functional.softmax(attention_score, dim=1)\n",
    "        \n",
    "        # Calculate the context vector using attention value and hidden states\n",
    "        context_vector = torch.sum(hidden_states * attention_value, dim=1)\n",
    "        \n",
    "        return context_vector\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "\n",
    "encoder = Encoder()\n",
    "\n",
    "layer_types_to_check = [nn.Conv2d, nn.Linear]\n",
    "\n",
    "for layer_type in layer_types_to_check:\n",
    "    no_layer = True\n",
    "    for child in encoder.children():\n",
    "        for layer in child.modules():\n",
    "            if(isinstance(layer, layer_type)):\n",
    "                no_layer = False\n",
    "    assert no_layer is False\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, y = next(loader_iter)\n",
    "print(encoder(x, masks).shape)\n",
    "print(round(encoder(x, masks)[0][0].item(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.938362Z",
     "start_time": "2022-02-21T03:00:13.923937Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1414316b7b53741b345aa834d1eebc82",
     "grade": true,
     "grade_id": "cell-db5f84f39a074460",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "encoder = Encoder()\n",
    "\n",
    "layer_types_to_check = [nn.Conv2d, nn.Linear]\n",
    "\n",
    "for layer_type in layer_types_to_check:\n",
    "    no_layer = True\n",
    "    for child in encoder.children():\n",
    "        for layer in child.modules():\n",
    "            if(isinstance(layer, layer_type)):\n",
    "                no_layer = False\n",
    "    assert no_layer is False\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, y = next(loader_iter)\n",
    "assert encoder(x, masks).shape == (10, 128), \"output shape is not (batch_size, 128)!\"\n",
    "assert round(encoder(x, masks)[0][0].item(), 4) == -0.1248, \"output value is wrong!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9f32cdb2ea9e1d0704a29e76fadb262",
     "grade": false,
     "grade_id": "cell-4456b6dc45c4375c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.2 Build the decoder model [15 points]\n",
    "\n",
    "Next, we'll build our decoder, which will be a GRU.\n",
    "\n",
    "The `Decoder` class does a single step of decoding, i.e. it ouputs single reconstructed results per time-step. The GRU layer will receive the initial hidden state from the encoder outputs. The first input to the decoder will be a zero vector, and then the decoder will try to reconstruct the input at timestep 1. For the following timestep, the input will be the output of the decoder from last timestep.\n",
    "\n",
    "Within the `forward` method, we accept a batch of inputs and previous hidden states. As we are only decoding one token at a time, the input tokens will always have a sequence length of 1. We `unsqueeze` the inputs to add a length dimension of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.947303Z",
     "start_time": "2022-02-21T03:00:13.940148Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: Define the RNN using `nn.GRU()`; \n",
    "              The `input_size` is 105. Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        self.rnn = nn.GRU(input_size=105, hidden_size=128, batch_first=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(128, 128)\n",
    "        self.act = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, 105)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, hiddens):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # diagnosis codes)\n",
    "            hiddens: the padding masks of shape (batch_size, 128)\n",
    "\n",
    "        Outputs:\n",
    "            preds: the reconstructed results of shape (batch_size, # diagnosis codes)\n",
    "            hiddens: the hidden state of the GRU with shape (batch_size, 128)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unsqueeze the input at dimension 1 -> (batch_size, 1, # diagnosis codes)\n",
    "        x = x.unsqueeze(1)\n",
    "        # Unsqueeze the hiddens at dimension 0; -> (1, batch_size, 128)\n",
    "        hiddens = hiddens.unsqueeze(0)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            1. Pass the input through the RNN layer; Remember to set the initial hidden states of GRU `h_0` to `hiddens`\n",
    "            2. Squeeze the hidden states h_n of RNN at dimension 0.\n",
    "        \"\"\"\n",
    "        h_n = None\n",
    "        # your code here\n",
    "        _, h_n = self.rnn(x, hiddens)\n",
    "\n",
    "        h_n = h_n.squeeze(0)\n",
    "        \n",
    "        preds = self.linear1(h_n)\n",
    "        preds = self.act(preds)\n",
    "        preds = self.linear2(preds)\n",
    "        preds = self.sigmoid(preds)\n",
    "        return preds, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.963350Z",
     "start_time": "2022-02-21T03:00:13.949475Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87b1b20c720dbe1dca84bac69430db61",
     "grade": true,
     "grade_id": "cell-fd2996e12ec715e9",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "decoder = Decoder()\n",
    "\n",
    "layer_types_to_check = [nn.GRU, nn.Linear, nn.Sigmoid]\n",
    "\n",
    "for layer_type in layer_types_to_check:\n",
    "    no_layer = True\n",
    "    for child in decoder.children():\n",
    "        for layer in child.modules():\n",
    "            if(isinstance(layer, layer_type)):\n",
    "                no_layer = False\n",
    "    assert no_layer is False\n",
    "\n",
    "x = torch.randn(10, 105)\n",
    "hiddens = torch.randn(10, 128)\n",
    "assert decoder(x, hiddens)[0].shape == (10, 105), \"preds is not of shape (batch_size, 105)!\"\n",
    "assert decoder(x, hiddens)[1].shape == (10, 128), \"h_n is not of shape (batch_size, 128)!\"\n",
    "assert round(decoder(x, hiddens)[0][0][0].item(), 4) == 0.4646, \"preds value is wrong!\"\n",
    "assert round(decoder(x, hiddens)[1][0][0].item(), 4) == 0.9358, \"h_n value is wrong!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T20:09:29.897381Z",
     "start_time": "2020-12-06T20:09:29.895194Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6202aef7e6430648e92eb0a03c5e2b90",
     "grade": false,
     "grade_id": "cell-40bad955f606721f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.3 Connect the encoder and decoder [20 points]\n",
    "\n",
    "For the final part of the implemenetation, we'll implement the Seq2Seq model\n",
    "\n",
    "The `Seq2Seq` model takes in an `Encoder` and a `Decoder`.\n",
    "\n",
    "Our `forward` method takes the source sequence, target sentence and a teacher-forcing ratio. The teacher forcing ratio is used when training our model. With probability equal to the teaching forcing ratio (`teacher_forcing_ratio`) we will use the actual ground-truth input in the sequence as the input to the decoder during the next time-step. However, with probability `1 - teacher_forcing_ratio`, we will use the output that the model predicted as the next input to the model.  \n",
    "\n",
    "The first input to the decoder is a zero vector.\n",
    "\n",
    "During each iteration of the loop, we:\n",
    "- pass the input, previous hidden states into the decoder\n",
    "- receive a prediction and next hidden state from the decoder\n",
    "- decide if we are going to \"teacher force\" or not\n",
    "    - if we do, the next `input` is the ground-truth input in the sequence\n",
    "    - if we don't, the next `input` is the predicted input in the sequence\n",
    "    \n",
    "Once we've made all of our predictions, we return our tensor full of predictions.\n",
    "\n",
    "For example, So our `inputs` and `outputs` of the decoder look something like ($x_t$ is the orinal input, $\\hat{x}_t$ is the reconstructed input):\n",
    "\n",
    "If not use teach force:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{inputs} &= [0, \\hat{x}_1, \\hat{x}_2]\\\\\n",
    "\\text{outputs} &= [\\hat{x}_1, \\hat{x}_2, \\hat{x}_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "If use teach force:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{inputs} &= [0, x_1, x_2]\\\\\n",
    "\\text{outputs} &= [\\hat{x}_1, \\hat{x}_2, \\hat{x}_3]\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.973453Z",
     "start_time": "2022-02-21T03:00:13.965943Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, teacher_forcing_ratio):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        \n",
    "    def forward(self, x, masks):\n",
    "        \n",
    "        \"\"\"\n",
    "        STEP:\n",
    "            1. Pass the input through the encoder; save the output to `hidden`;\n",
    "            2. Pass the first input (0) and encoder hidden state to the decoder;\n",
    "            3. Use for-loop to \n",
    "                   1. Use `random.random()` to decide whether to use teacher force\n",
    "                   2. Pass the previous output `cur_pred` / the ground truth input `x[:, t-1, :]` \n",
    "                      and previous hidden state `h` to the decoder;\n",
    "                   3. Save `cur_pred`, the output of the decoder, to a list.\n",
    "            4. Use torch.stack to convert the list to a tensor and \n",
    "               reshape it to (batch_size, # visits, # diagnosis codes)\n",
    "            \n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits)\n",
    "\n",
    "        Outputs:\n",
    "            preds: the reconstructed results of shape (batch_size, # visits, # diagnosis codes)\n",
    "        \"\"\"\n",
    "        \n",
    "        preds = []\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: perform step 1\n",
    "        \"\"\"\n",
    "        hidden = self.encoder(x, masks)\n",
    "\n",
    "        \n",
    "        # step 2\n",
    "        x_decode = torch.zeros(batch_size, 105)\n",
    "        cur_pred, h = self.decoder(x_decode, hidden)\n",
    "        preds.append(cur_pred)\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            # step 3.1\n",
    "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
    "            \n",
    "            \"\"\"\n",
    "            TODO: perform step 3.2\n",
    "            \"\"\"\n",
    "            # your code here\n",
    "            if teacher_force:\n",
    "                # Step 3.2: Pass the ground truth input and previous hidden state to the decoder\n",
    "                cur_pred, h = self.decoder(x[:, t - 1, :], h)\n",
    "            else:\n",
    "                # Step 3.2: Pass the previous output and previous hidden state to the decoder\n",
    "                cur_pred, h = self.decoder(cur_pred, h)\n",
    "            \n",
    "            # step 3.3\n",
    "            preds.append(cur_pred)\n",
    "        \n",
    "        # step 4\n",
    "        return torch.stack(preds).permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.994072Z",
     "start_time": "2022-02-21T03:00:13.975872Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffc66d675f37bafbadb62f3e20d8ed7f",
     "grade": true,
     "grade_id": "cell-06fd2b774fea5691",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "seq2seq = Seq2Seq(encoder, decoder, 0.5)\n",
    "\n",
    "layer_types_to_check = [nn.GRU, nn.Linear, nn.Sigmoid]\n",
    "\n",
    "for layer_type in layer_types_to_check:\n",
    "    no_layer = True\n",
    "    for child in seq2seq.children():\n",
    "        for layer in child.modules():\n",
    "            if(isinstance(layer, layer_type)):\n",
    "                no_layer = False\n",
    "    assert no_layer is False\n",
    "\n",
    "x = torch.randn(10, 5, 105)\n",
    "masks = torch.ones(10, 5, dtype=torch.bool)\n",
    "assert seq2seq(x, masks).shape == (10, 5, 105), \"output shape should be (batch_size, # visits, # diagnosis codes)!\"\n",
    "assert round(seq2seq(x, masks)[0][0][0].item(), 4) == 0.4819, \"output value is wrong!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3660c73a4ab361282394fd113086c34c",
     "grade": false,
     "grade_id": "cell-be62492fb269883c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4 Model training [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b76222840b264add16720ea2eae3e3a",
     "grade": false,
     "grade_id": "cell-055e3d88da155e04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.1 Loss and optimizer\n",
    "Because we are reconstructing original input, we use BCELoss as loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:13.999706Z",
     "start_time": "2022-02-21T03:00:13.995936Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "794c6946b8e66b876094348abf55ccc6",
     "grade": false,
     "grade_id": "cell-608d0514fef945d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set reduction='none' because we need to mask out the padding values manually.\n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:38:05.210804Z",
     "start_time": "2020-12-07T13:38:05.207769Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2bf00c543b0fdd07d28afd25b735769",
     "grade": false,
     "grade_id": "cell-49b8e14db7d29817",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:00:14.411001Z",
     "start_time": "2022-02-21T03:00:14.001833Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e6129b31d7de272b75ee30c4c9bd067",
     "grade": false,
     "grade_id": "cell-aa7e17a496e35c5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "def eval(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the seq2seq model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        f1: overall f1 score\n",
    "        accuracy: overall accuracy score; \n",
    "        use np.round to round the preds to calculate the metrics.\n",
    "        \n",
    "    We have implement this for you.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    x_pred = []\n",
    "    x_true = []\n",
    "    for x, masks, y in val_loader:\n",
    "        x_hat = model(x, masks).detach().numpy()\n",
    "        x_hat = np.round(x_hat)\n",
    "        for i in range(len(x_hat)):\n",
    "            for j in range(len(x_hat[i])):\n",
    "                if masks[i, j] == 1:\n",
    "                    x_pred.append(x_hat[i,j])\n",
    "                    x_true.append(x.numpy()[i,j])\n",
    "    \n",
    "    f = f1_score(y_pred=x_pred, y_true=x_true, average='micro')\n",
    "    acc = accuracy_score(y_pred=x_pred, y_true=x_true)\n",
    "    return f, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27866387e1328de2c8c617c3cc8b9342",
     "grade": false,
     "grade_id": "cell-6a0ef300a62e964f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:01:04.190822Z",
     "start_time": "2022-02-21T03:00:14.412701Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "935dc2777859ddd8fc272ad7bd3248e1",
     "grade": false,
     "grade_id": "cell-f1bd817117170596",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    We have implement this for you.\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        seq2seq.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_hat = seq2seq(x, masks)\n",
    "            loss = criterion(x_hat, x)\n",
    "            loss = loss * masks.float().unsqueeze(-1)\n",
    "            loss = torch.mean(torch.sum(loss, dim=[1,2]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        f, acc = eval(model, val_loader)\n",
    "        print('Epoch: %d \\t Validation f: %.2f, acc: %.2f'%(epoch+1, f, acc))\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "train(seq2seq, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:01:04.276077Z",
     "start_time": "2022-02-21T03:01:04.192510Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ab6e8526f6d5f5b963c3ef9965b7001",
     "grade": true,
     "grade_id": "cell-1bcd0ee830fdae35",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "941dd7511538bd431f756112754a62b6",
     "grade": false,
     "grade_id": "cell-6478a0ab26954d0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5 Unsupervised patient clustering [15 points]\n",
    "\n",
    "After training a seq2seq model, we can use the encoder to generate patient embeddings. These embeddings can be used for unsupervised tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:01:04.321915Z",
     "start_time": "2022-02-21T03:01:04.277883Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db178ab534de4ead712cbe04ef136dea",
     "grade": false,
     "grade_id": "cell-cfc8376c4f541b57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(model, data_loader):\n",
    "    \"\"\"\n",
    "    Use trained encoder model to get patient embeddings.\n",
    "    \n",
    "    Arguments:\n",
    "        model: trained seq2seq model\n",
    "        data_loader: dataloder\n",
    "        \n",
    "    Return:\n",
    "        embeddings: numpy array that contains patient embeddings.\n",
    "        labels: numpy array that contains patient heart failure label.\n",
    "    \n",
    "    We have implement this for you.\n",
    "    \"\"\"\n",
    "    \n",
    "    seq2seq.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    for x, masks, y in data_loader:\n",
    "        cur_embd = seq2seq.encoder(x, masks)\n",
    "        embeddings += list(cur_embd.detach().numpy())\n",
    "        labels += list(y.numpy())\n",
    "        \n",
    "    return np.array(embeddings), np.array(labels)\n",
    "\n",
    "    \n",
    "embeddings, labels = get_embeddings(seq2seq, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3138d4870f60bffbe8f26e772d230af9",
     "grade": false,
     "grade_id": "cell-1bb215c283b707df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next we will use KMeans algorithm to cluster these embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:01:04.684419Z",
     "start_time": "2022-02-21T03:01:04.323593Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_clusters(embeddings):\n",
    "    \"\"\"\n",
    "    TODO: 1. Use K-means to generate patient clusters. (set K to 2)\n",
    "          2. Use calinski harabaz score to evaluate clustering results\n",
    "    \n",
    "    Arguments:\n",
    "        embeddings: obtained patient embeddings\n",
    "        \n",
    "    Return:\n",
    "        labels: K-means clustering labels\n",
    "        \n",
    "    Hint: Use sklearn.cluster.KMeans and sklearn.metrics.calinski_harabasz_score.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "cluster_labels = get_clusters(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:01:04.725228Z",
     "start_time": "2022-02-21T03:01:04.687524Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb02afdf21c7c6061dbb97a14629dbd0",
     "grade": true,
     "grade_id": "cell-766460c1932d6947",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "assert cluster_labels.shape == (200, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69e95d703f47480cb8196dcc419d7e1f",
     "grade": false,
     "grade_id": "cell-e953823d84417c44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us use T-SNE to visualize the embeddings and clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-21T03:01:06.134084Z",
     "start_time": "2022-02-21T03:01:04.727269Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32e9f3b56949f6c340a7cd40b3ecf509",
     "grade": false,
     "grade_id": "cell-c84efd7951adb169",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=42)\n",
    "tsne_embd = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.scatter(tsne_embd[:,0], tsne_embd[:,1], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "115735480aaaffe19cb82f4c649c6bc0",
     "grade": false,
     "grade_id": "cell-0033092e6ce467a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The cluster result is not really good due to the limited resources. Feel free to explore more on this topic in project with larger dataset."
   ]
  }
 ],
 "metadata": {
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/HW3_Seq2Seq/HW3_Seq2Seq.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (Threads: 2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
